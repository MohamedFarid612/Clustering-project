{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection using K-Means and Spectral Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second: Importing the dataset and preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('archive/kddcup.data.corrected', header=None)\n",
    "\n",
    "# Splitting the data into features and labels. The last column is the label\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels:  23\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique labels: ', len(y.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third: Applying K-Means and Spectral Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suplementary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to calculate the euclidean distance between two data points\n",
    "# Since the data is a mix of categorical and numerical data, we will use the following formula:\n",
    "# d = sqrt( (x1 - x2)^2 + (y1 - y2)^2 + ... + (z1 - z2)^2 + (a1 == a2)^2 + (b1 == b2)^2 + (c1 == c2)^2 )\n",
    "# where x, y, z are numerical features and a, b, c are categorical features\n",
    "def euclidean_distance( row, centroid, data ):\n",
    "    distance = 0.0\n",
    "    for column in data.columns:\n",
    "        try:\n",
    "            distance += (row[column] - centroid[column])**2\n",
    "        except:\n",
    "            distance += (row[column] == centroid[column])**2\n",
    "\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "\n",
    "# This function will be used to calculate the mean of the data points in a cluster.\n",
    "# Our data contains both numerical and categorical data. So, we need to handle them separately.\n",
    "# For numerical data, we take the mean of the data points in the cluster.\n",
    "# For categorical data, we take the mode of the data points in the cluster. (Most frequent value)\n",
    "def calculate_mean( data ):\n",
    "    mean = pd.DataFrame(columns=data.columns)\n",
    "    for column in data.columns:\n",
    "        try:\n",
    "            mean[column] = [data[column].mean()]\n",
    "        except:\n",
    "            mean[column] = [data[column].mode()[0]]\n",
    "    return mean\n",
    "    \n",
    "\n",
    "# This function returns the ANSI code for bold text\n",
    "def bold( text, reset=True ):\n",
    "    if reset:\n",
    "        return '\\033[1m' + text + '\\033[0m'\n",
    "    return '\\033[1m' + text\n",
    "\n",
    "# This function returns the ANSI code for underlined text\n",
    "def underline( text, reset=True ):\n",
    "    if reset:\n",
    "        return '\\033[4m' + text + '\\033[0m'\n",
    "    return '\\033[4m' + text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering( k, data, max_iterations:int=None, print_updates=False ):\n",
    "    \n",
    "    # Initially, selecting k random data points as centroids\n",
    "    # We will use the current time as the seed to make sure that we get different centroids each time we run the algorithm\n",
    "    np.random.seed( int(time.time()) )\n",
    "    centroids = data.sample(k)\n",
    "    old_centroids = None\n",
    "\n",
    "    if max_iterations is None:\n",
    "        max_iterations = np.inf\n",
    "\n",
    "    itr = 1\n",
    "\n",
    "    # If the user doesn't specify the maximum number of iterations, we will set it to infinity (Loop until convergence)\n",
    "    while( itr <= max_iterations ):\n",
    "\n",
    "        # If the centroids do not change, we will stop the algorithm\n",
    "        if centroids.equals( old_centroids ):\n",
    "            break\n",
    "\n",
    "        # Storing the old centroids to check if they change in the next iteration\n",
    "        old_centroids = centroids.copy()\n",
    "\n",
    "        if print_updates is True: print( underline(bold('Iteration #' + str(itr), reset=False)) )\n",
    "\n",
    "        # Creating a dictionary to store the clusters and their data points\n",
    "        # 'clusters' will store the data points and 'cluster_indices' will store their indices\n",
    "        clusters, cluster_indices = {}, {}\n",
    "        for i in range(k):\n",
    "            clusters[i] = []\n",
    "            cluster_indices[i] = []\n",
    "\n",
    "        # Iterating through each data point and assigning it to the closest cluster\n",
    "        for index, row in data.iterrows():\n",
    "\n",
    "            min_distance, closest_cluster_index = np.inf, -1\n",
    "            \n",
    "            # Iterating through each centroid to find the closest one\n",
    "            for i in range(k):\n",
    "\n",
    "                # Using our euclidean_distance function to calculate the distance as it handles both numerical and categorical data\n",
    "                current_distance = euclidean_distance( row, centroids.iloc[i], data )\n",
    "\n",
    "                # Check if the data point is closer to the ith centroid\n",
    "                if current_distance < min_distance:\n",
    "                    min_distance = current_distance\n",
    "                    closest_cluster_index = i\n",
    "            \n",
    "            # Assigning the data point to the cluster with the closest centroid\n",
    "            clusters[closest_cluster_index].append( row )\n",
    "            cluster_indices[closest_cluster_index].append( index )\n",
    "\n",
    "        # Updating the centroids.\n",
    "        # We will use our calculate_mean function to calculate the mean of the data points in the cluster\n",
    "        # because it handles both numerical and categorical data\n",
    "        for i in range(k):\n",
    "\n",
    "            # If the cluster is empty, we will not update the centroid\n",
    "            if len(clusters[i]) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                centroids.iloc[i] = calculate_mean( pd.DataFrame(clusters[i]) )\n",
    "\n",
    "        if print_updates is True:\n",
    "            print( 'Cluster sizes:' )\n",
    "            print(pd.DataFrame( [len(clusters[i]) for i in range(k)] ).T)\n",
    "            print()\n",
    "\n",
    "        if print_updates is True: print('-'*50) # Just to print a line to separate the iterations\n",
    "\n",
    "        itr += 1\n",
    "\n",
    "    return centroids, clusters, cluster_indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to calculate the purity of the clusters\n",
    "# Purity is the percentage of data points in a cluster that belong to the same class\n",
    "def calculate_purity( clusters, labels, print_report=False ):\n",
    "    purity = 0.0\n",
    "    purities = []\n",
    "    for i in range( len(clusters) ):\n",
    "        cluster = clusters[i]\n",
    "\n",
    "        # If the cluster is empty, we will skip it\n",
    "        if len(cluster) == 0: continue\n",
    "\n",
    "        # Converting the cluster to a dataframe so that we can use the value_counts() function\n",
    "        cluster = pd.DataFrame(cluster)\n",
    "        cluster['label'] = labels[cluster.index]\n",
    "\n",
    "        # We will use the value_counts() function to count the number of data points in each class\n",
    "        # and then we will divide it by the total number of data points in the cluster\n",
    "        purities.append( cluster['label'].value_counts()[0] / len(cluster) )\n",
    "\n",
    "    # Normalizing the purity by dividing it by the number of clusters\n",
    "    average_purity = sum(purities) / len(clusters)\n",
    "\n",
    "    if print_report is True:\n",
    "        for i in range(len(purities)):\n",
    "            print('Cluster ', i+1, ' purity: ', purities[i])\n",
    "        print('-'*50)\n",
    "        print('Average Purity: ', average_purity)\n",
    "        print('-'*50)\n",
    "    \n",
    "    return average_purity, purities\n",
    "\n",
    "\n",
    "# This function prints a report of the clusters produced by the k-means algorithm\n",
    "def analyze_clusters( clusters, cluster_indices, labels ):\n",
    "\n",
    "    # Printing the number of data points in each cluster\n",
    "    for i in range(len(cluster_indices)):\n",
    "        print('Cluster ', i+1, ' contains ', len(cluster_indices[i]), ' data points')\n",
    "    print( '-'*50 )\n",
    "\n",
    "    # Calculating the purity of the clusters and printing the report\n",
    "    calculate_purity( clusters, labels, print_report=True )\n",
    "\n",
    "    # Printing the number of unique labels in each cluster\n",
    "    for i in range(len(cluster_indices)):\n",
    "        print( underline(bold('Cluster', i+1, 'labels:\\n')) )\n",
    "        print(y_test[cluster_indices[i]].value_counts())\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1mIteration #1\u001b[0m\n",
      "14974\n",
      "34011\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1mIteration #2\u001b[0m\n",
      "\u001b[91mProcess interrupted by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    centroids23, clusters23, cluster_indices23 = kmeans_clustering( k=2, data=X_test, max_iterations=5, print_updates=True )\n",
    "except KeyboardInterrupt:\n",
    "    print('\\033[91m' + 'Process interrupted by user' + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analyze_clusters( clusters23, cluster_indices23, y_test )\n",
    "except:\n",
    "    print('\\033[91m' + 'Error: No clusters found' + '\\033[0m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
