{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection using K-Means and Spectral Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second: Importing the dataset and preprocessing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df = pd.read_csv('archive/kddcup.data.corrected', header=None)\n",
    "\n",
    "# Splitting the data into features and labels. The last column is the label\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels:  23\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique labels: ', len(y.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third: Applying K-Means and Spectral Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to calculate the euclidean distance between two data points\n",
    "# Since the data is a mix of categorical and numerical data, we will use the following formula:\n",
    "# d = sqrt( (x1 - x2)^2 + (y1 - y2)^2 + ... + (z1 - z2)^2 + (a1 == a2)^2 + (b1 == b2)^2 + (c1 == c2)^2 )\n",
    "# where x, y, z are numerical features and a, b, c are categorical features\n",
    "def euclidean_distance( row, centroid, data ):\n",
    "    distance = 0.0\n",
    "    for column in data.columns:\n",
    "        try:\n",
    "            distance += (row[column] - centroid[column])**2\n",
    "        except:\n",
    "            distance += (row[column] == centroid[column])**2\n",
    "\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "\n",
    "# This function will be used to calculate the mean of the data points in a cluster.\n",
    "# Our data contains both numerical and categorical data. So, we need to handle them separately.\n",
    "# For numerical data, we take the mean of the data points in the cluster.\n",
    "# For categorical data, we take the mode of the data points in the cluster. (Most frequent value)\n",
    "def calculate_mean( data ):\n",
    "    mean = pd.DataFrame(columns=data.columns)\n",
    "    for column in data.columns:\n",
    "        try:\n",
    "            mean[column] = [data[column].mean()]\n",
    "        except:\n",
    "            mean[column] = [data[column].mode()[0]]\n",
    "    return mean\n",
    "    \n",
    "\n",
    "# This function returns the ANSI code for bold text\n",
    "def bold( text, reset=True ):\n",
    "    if reset:\n",
    "        return '\\033[1m' + text + '\\033[0m'\n",
    "    return '\\033[1m' + text\n",
    "\n",
    "# This function returns the ANSI code for underlined text\n",
    "def underline( text, reset=True ):\n",
    "    if reset:\n",
    "        return '\\033[4m' + text + '\\033[0m'\n",
    "    return '\\033[4m' + text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering( k, data, max_iterations:int=None, print_updates=False, initial_centroids=None ):\n",
    "    \n",
    "    # Initially, selecting k random data points as centroids\n",
    "    # We will use the current time as the seed to make sure that we get different centroids each time we run the algorithm\n",
    "    np.random.seed( int(time.time()) )\n",
    "\n",
    "    if initial_centroids is None:\n",
    "        centroids = data.sample(k)\n",
    "    else:\n",
    "        centroids = initial_centroids.copy()\n",
    "        \n",
    "    old_centroids = None\n",
    "\n",
    "    # If the user doesn't specify the maximum number of iterations, we will set it to infinity (Loop until convergence)\n",
    "    if max_iterations is None:\n",
    "        max_iterations = np.inf\n",
    "\n",
    "    itr = 1\n",
    "    while( itr <= max_iterations ):\n",
    "\n",
    "        # If the centroids do not change, we will stop the algorithm\n",
    "        if centroids.equals( old_centroids ):\n",
    "            break\n",
    "\n",
    "        # Storing the old centroids to check if they change in the next iteration\n",
    "        old_centroids = centroids.copy()\n",
    "\n",
    "        if print_updates is True: print( underline(bold('Iteration #' + str(itr))) )\n",
    "\n",
    "        # Creating a dictionary to store the clusters and their data points\n",
    "        # 'clusters' will store the data points and 'cluster_indices' will store their indices\n",
    "        clusters, cluster_indices = {}, {}\n",
    "        for i in range(k):\n",
    "            clusters[i] = []\n",
    "            cluster_indices[i] = []\n",
    "\n",
    "        # Iterating through each data point and assigning it to the closest cluster\n",
    "        for index, row in data.iterrows():\n",
    "\n",
    "            min_distance, closest_cluster_index = np.inf, -1\n",
    "            \n",
    "            # Iterating through each centroid to find the closest one\n",
    "            for i in range(k):\n",
    "\n",
    "                # Using our euclidean_distance function to calculate the distance as it handles both numerical and categorical data\n",
    "                current_distance = euclidean_distance( row, centroids.iloc[i], data )\n",
    "\n",
    "                # Check if the data point is closer to the ith centroid\n",
    "                if current_distance < min_distance:\n",
    "                    min_distance = current_distance\n",
    "                    closest_cluster_index = i\n",
    "            \n",
    "            # Assigning the data point to the cluster with the closest centroid\n",
    "            clusters[closest_cluster_index].append( row )\n",
    "            cluster_indices[closest_cluster_index].append( index )\n",
    "\n",
    "        # Updating the centroids.\n",
    "        # We will use our calculate_mean function to calculate the mean of the data points in the cluster\n",
    "        # because it handles both numerical and categorical data\n",
    "        for i in range(k):\n",
    "\n",
    "            # If the cluster is empty, we will not update the centroid\n",
    "            if len(clusters[i]) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                centroids.iloc[i] = calculate_mean( pd.DataFrame(clusters[i]) )\n",
    "\n",
    "        # Printing the cluster sizes in tabular form to conserve vertical space\n",
    "        if print_updates is True:\n",
    "            print( 'Cluster sizes:' )\n",
    "            print(pd.DataFrame( [len(clusters[i]) for i in range(k)] ).T)\n",
    "\n",
    "        if print_updates is True: print('-'*50) # Just to print a line to separate the iterations\n",
    "\n",
    "        itr += 1\n",
    "\n",
    "    return centroids, clusters, cluster_indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to calculate the purity of the clusters\n",
    "# Purity is the percentage of data points in a cluster that belong to the same class\n",
    "def calculate_purity( clusters, labels, print_report=False ):\n",
    "    purity = 0.0\n",
    "    purities = []\n",
    "    for i in range( len(clusters) ):\n",
    "        cluster = clusters[i]\n",
    "\n",
    "        # If the cluster is empty, we will skip it\n",
    "        if len(cluster) == 0: continue\n",
    "\n",
    "        # Converting the cluster to a dataframe so that we can use the value_counts() function\n",
    "        cluster = pd.DataFrame(cluster)\n",
    "        cluster['label'] = labels[cluster.index]\n",
    "\n",
    "        # We will use the value_counts() function to count the number of data points in each class\n",
    "        # and then we will divide it by the total number of data points in the cluster\n",
    "        purities.append( cluster['label'].value_counts()[0] / len(cluster) )\n",
    "\n",
    "    # Normalizing the purity by dividing it by the number of clusters\n",
    "    average_purity = sum(purities) / len(clusters)\n",
    "\n",
    "    if print_report is True:\n",
    "        for i in range(len(purities)):\n",
    "            print('Cluster ', i+1, ' purity: ', purities[i])\n",
    "        print('-'*50)\n",
    "        print('Average Purity: ', average_purity)\n",
    "        print('-'*50)\n",
    "    \n",
    "    return average_purity, purities\n",
    "\n",
    "\n",
    "# This function prints a report of the clusters produced by the k-means algorithm\n",
    "def analyze_clusters( clusters, cluster_indices, labels ):\n",
    "\n",
    "    # Printing the number of data points in each cluster\n",
    "    for i in range(len(cluster_indices)):\n",
    "        print('Cluster ', i+1, ' contains ', len(cluster_indices[i]), ' data points')\n",
    "    print( '-'*50 )\n",
    "\n",
    "    # Calculating the purity of the clusters and printing the report\n",
    "    calculate_purity( clusters, labels, print_report=True )\n",
    "\n",
    "    # Printing the number of unique labels in each cluster\n",
    "    for i in range(len(cluster_indices)):\n",
    "        print( 'Cluster #' + str(i+1) + ' labels:\\n' )\n",
    "        print(labels[cluster_indices[i]].value_counts())\n",
    "        print('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1mIteration #1\u001b[0m\n",
      "Cluster sizes:\n",
      "      0     1      2     3      4    5  6     7\n",
      "0  2999  1501  23259  4203  12550  314  0  4159\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1mIteration #2\u001b[0m\n",
      "Cluster sizes:\n",
      "       0    1    2     3     4   5      6     7\n",
      "0  10791  480  846  1613  7284  36  22555  5380\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1mIteration #3\u001b[0m\n",
      "Cluster sizes:\n",
      "       0     1    2    3     4  5      6     7\n",
      "0  14050  1121  331  856  4632  5  22602  5388\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1mIteration #4\u001b[0m\n",
      "Cluster sizes:\n",
      "       0     1    2    3     4  5      6     7\n",
      "0  15722  1315  160  456  3341  4  22599  5388\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1mIteration #5\u001b[0m\n",
      "Cluster sizes:\n",
      "       0     1   2    3     4  5      6     7\n",
      "0  16952  1356  91  241  2354  4  22599  5388\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    centroids8, clusters8, cluster_indices8 = kmeans_clustering( k=8, data=X_test, max_iterations=5, print_updates=True )\n",
    "except KeyboardInterrupt:\n",
    "    print('\\033[91m' + 'Process interrupted by user' + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  1  contains  16952  data points\n",
      "Cluster  2  contains  1356  data points\n",
      "Cluster  3  contains  91  data points\n",
      "Cluster  4  contains  241  data points\n",
      "Cluster  5  contains  2354  data points\n",
      "Cluster  6  contains  4  data points\n",
      "Cluster  7  contains  22599  data points\n",
      "Cluster  8  contains  5388  data points\n",
      "--------------------------------------------------\n",
      "Cluster  1  purity:  0.6390396413402548\n",
      "Cluster  2  purity:  0.9837758112094396\n",
      "Cluster  3  purity:  0.7912087912087912\n",
      "Cluster  4  purity:  1.0\n",
      "Cluster  5  purity:  0.9944774851316908\n",
      "Cluster  6  purity:  0.5\n",
      "Cluster  7  purity:  1.0\n",
      "Cluster  8  purity:  0.999072011878248\n",
      "--------------------------------------------------\n",
      "Average Purity:  0.863446717596053\n",
      "--------------------------------------------------\n",
      "Cluster #1 labels:\n",
      "\n",
      "neptune.        10833\n",
      "normal.          5737\n",
      "satan.            146\n",
      "ipsweep.          109\n",
      "portsweep.         92\n",
      "nmap.              17\n",
      "teardrop.          13\n",
      "warezclient.        4\n",
      "smurf.              1\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "Cluster #2 labels:\n",
      "\n",
      "normal.         1334\n",
      "smurf.            16\n",
      "pod.               4\n",
      "warezclient.       2\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "Cluster #3 labels:\n",
      "\n",
      "normal.    72\n",
      "back.      19\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "Cluster #4 labels:\n",
      "\n",
      "normal.    241\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "Cluster #5 labels:\n",
      "\n",
      "normal.         2341\n",
      "portsweep.        10\n",
      "warezclient.       3\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "Cluster #6 labels:\n",
      "\n",
      "warezclient.    2\n",
      "normal.         2\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "Cluster #7 labels:\n",
      "\n",
      "smurf.    22599\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "Cluster #8 labels:\n",
      "\n",
      "smurf.     5383\n",
      "normal.       5\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    analyze_clusters( clusters8, cluster_indices8, y_test )\n",
    "except NameError:\n",
    "    print('\\033[91m' + 'Error: No clusters found' + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    centroids23, clusters23, cluster_indices23 = kmeans_clustering( k=23, data=X_test, max_iterations=5, print_updates=True )\n",
    "except KeyboardInterrupt:\n",
    "    print('\\033[91m' + 'Process interrupted by user' + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analyze_clusters( clusters23, cluster_indices23, y_test )\n",
    "except NameError:\n",
    "    print('\\033[91m' + 'Error: No clusters found' + '\\033[0m')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Cut Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation (Not vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function measures the normalized cut of a graph, given the weight matrix and the clusters resulting from the graph cut\n",
    "# weight_matrix expects a numpy array, and clusters expects a list of lists where each list contains the indices of the nodes in the cluster\n",
    "def measure_cut(weight_matrix, clusters):\n",
    "    \n",
    "    # Calculating the cut\n",
    "    # Here, we will iterate through each pair of clusters and calculate the sum of the weights of the edges between them\n",
    "    # We add the cut between the ith and jth clusters to the total cut measure which will then be used to calculate the normalized cut\n",
    "    total_cut_measure = 0\n",
    "    for i in range(len(clusters)):\n",
    "        for j in range(len(clusters)):\n",
    "\n",
    "            # We don't want to calculate the cut between a cluster and itself\n",
    "            # So, we will skip the iteration if i == j\n",
    "            if i == j: continue\n",
    "\n",
    "            # Calculating the cut between the ith and jth clusters\n",
    "            cut += np.sum( weight_matrix[clusters[i], :][:, clusters[j]] )\n",
    "\n",
    "    # Calculating the volume\n",
    "    # Here, we will iterate through each cluster and calculate the sum of the weights of the edges inside the cluster\n",
    "    total_volume = 0\n",
    "    for i in range(len(clusters)):\n",
    "        total_volume += np.sum( weight_matrix[clusters[i], :][:, clusters[i]] )\n",
    "\n",
    "    # Calculating the normalized cut\n",
    "    normalized_cut = total_cut_measure / total_volume\n",
    "\n",
    "    return normalized_cut, total_cut_measure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
