{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for text formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESET = \"\\033[0m\"\n",
    "BOLD = \"\\033[1m\"\n",
    "UNDERLINE = \"\\033[4m\"\n",
    "COLOR_RED = \"\\033[31m\"\n",
    "COLOR_GREEN = \"\\033[32m\"\n",
    "COLOR_CYAN = \"\\033[36m\"\n",
    "\n",
    "def textf(text, format):\n",
    "    return f\"{format}{text}{RESET}\"\n",
    "\n",
    "def bold(text):\n",
    "    return textf(text, BOLD)\n",
    "\n",
    "def underline(text):\n",
    "    return textf(text, UNDERLINE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection using K-Means and Spectral Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second: Importing the dataset and preprocessing it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We first load the data from the CSVs.\n",
    "- We then proceed with preprocessing the data as follows:\n",
    "    1. Split the labels from the data (last column) and place them in separate dataframes.\n",
    "    2. Encode the categorical data using into one-hot vectors using the OneHotEncoder class.\n",
    "    3. Scale the features using the StandardScaler class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> tuple:\n",
    "\n",
    "    # Loading the data as indicated in the assignment\n",
    "    # kddcup.data.corrected is the training data, corrected is the testing data\n",
    "    print('Loading the data...', end=' ')\n",
    "    training_data = pd.read_csv( 'archive/kddcup.data_10_percent_corrected', header=None )\n",
    "    testing_data = pd.read_csv( 'archive/corrected', header=None )\n",
    "    print( textf('Done!', COLOR_GREEN) )\n",
    "\n",
    "    # Separating the labels (last column) from the features in the training and testing data\n",
    "    print('Separating the labels from the features...', end=' ')\n",
    "    X_train = training_data.iloc[:, :-1]\n",
    "    y_train = training_data.iloc[:, -1]\n",
    "\n",
    "    X_test = testing_data.iloc[:, :-1]\n",
    "    y_test = testing_data.iloc[:, -1]\n",
    "    print( textf('Done!', COLOR_GREEN) )\n",
    "\n",
    "    # Concatenating the training and testing data vertically to perform OneHotEncoding on all the categorical features\n",
    "    print('Concatenating the training and testing data...', end=' ')\n",
    "    X = pd.concat( [X_train, X_test], axis=0 )\n",
    "    print( textf('Done!', COLOR_GREEN) )\n",
    "\n",
    "    # Encoding the features into one-hot vectors using OneHotEncoder\n",
    "    # ColumnTransformer is used to apply the OneHotEncoder to the second, third and fourth columns (categorical features)\n",
    "    # The remainder is set to 'passthrough' to keep the other columns unchanged (already numerical)\n",
    "    print('Encoding the features into one-hot vectors...', end=' ')\n",
    "    ct = ColumnTransformer( [('one_hot_encoder', OneHotEncoder(), [1, 2, 3])], remainder='passthrough' )\n",
    "    ct = ct.fit(X)\n",
    "    X_train = pd.DataFrame( ct.transform(X_train) )\n",
    "    X_test = pd.DataFrame( ct.transform(X_test) )\n",
    "    print( textf('Done!', COLOR_GREEN) )\n",
    "\n",
    "    # Feature Scaling since some features have a much higher range than others\n",
    "    print('Feature Scaling...', end=' ')\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame( scaler.fit_transform( X_train ) )\n",
    "    X_test = pd.DataFrame( scaler.transform( X_test ) )\n",
    "    print( textf('Done!', COLOR_GREEN) )\n",
    "\n",
    "    print('All done!')\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Randomly picking some samples from the training data to speed up the training process (for testing purposes only)\n",
    "def sample_data(X_train, y_train, X_test, y_test, n_train:int=10000, n_test:int=1000) -> tuple:\n",
    "    print('Sampling the data...', end=' ')\n",
    "    X_train = X_train.sample(n_train, random_state= 42)\n",
    "    y_train = y_train[X_train.index]\n",
    "\n",
    "    X_test = X_test.sample(n_test, random_state= 42)\n",
    "    y_test = y_test[X_test.index]\n",
    "    print( textf('Done!', COLOR_GREEN) )\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data... \u001b[32mDone!\u001b[0m\n",
      "Separating the labels from the features... \u001b[32mDone!\u001b[0m\n",
      "Concatenating the training and testing data... \u001b[32mDone!\u001b[0m\n",
      "Encoding the features into one-hot vectors... \u001b[32mDone!\u001b[0m\n",
      "Feature Scaling... \u001b[32mDone!\u001b[0m\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Uncomment the following line to sample the data (for testing purposes only)\n",
    "\n",
    "# X_train, y_train, X_test, y_test = sample_data(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in the training data:  23\n",
      "--------------------------------------------------\n",
      "Number of unique labels in the testing data :  38\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Number of unique labels in the training data: ', len(y_train.unique()))\n",
    "    print('-' * 50)\n",
    "    print('Number of unique labels in the testing data : ', len(y_test.unique()))\n",
    "except NameError:\n",
    "    print( textf('NameError: y_train or y_test is not defined', COLOR_RED) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third: Applying K-Means and Normalized Cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# 1. Precision\n",
    "def precision(cluster_indices, cluster_labels, y_test):\n",
    "    total_precision = 0\n",
    "    for i in range( len(cluster_indices) ):\n",
    "        # Calculating the precision of the ith cluster\n",
    "        precision = np.sum( cluster_labels[i] == y_test[cluster_indices[i]] ) / len(cluster_indices[i])\n",
    "        total_precision += precision\n",
    "            \n",
    "    average_precision = total_precision / len(cluster_indices)\n",
    "    return average_precision\n",
    "\n",
    "# 2. Recall\n",
    "def recall(cluster_indices, cluster_labels, y_test):\n",
    "    total_recall = 0\n",
    "    for i in range( len(cluster_indices) ):\n",
    "        # Calculating the recall of the ith cluster\n",
    "        recall = np.sum( cluster_labels[i] == y_test[cluster_indices[i]] ) / len(y_test[cluster_indices[i]])\n",
    "        total_recall += recall\n",
    "        \n",
    "    average_recall = total_recall / len(cluster_indices)\n",
    "    return average_recall\n",
    "\n",
    "# 3. F1 Score\n",
    "def F1_score(cluster_indices, cluster_labels, y_test):\n",
    "    # Calculating the F1 score\n",
    "    # Here, we will iterate through each cluster and calculate the F1 score of the cluster\n",
    "    # We add the F1 score of the ith cluster to the total F1 score which will then be used to calculate the average F1 score\n",
    "    total_F1_score = 0\n",
    "    for i in range( len(cluster_indices) ):\n",
    "        # Calculating the F1 score of the ith cluster\n",
    "        precision = precision(cluster_indices, cluster_labels, y_test)\n",
    "        recall = recall(cluster_indices, cluster_labels, y_test)\n",
    "        F1_score = 2 * precision * recall / (precision + recall)\n",
    "        total_F1_score += F1_score\n",
    "\n",
    "    # Calculating the average F1 score\n",
    "    average_F1_score = total_F1_score / len(cluster_indices)\n",
    "\n",
    "    return average_F1_score\n",
    "# 4. Conditional Entropy\n",
    "def conditional_entropy(cluster_indices, cluster_labels, y_test):\n",
    "    return True\n",
    "\n",
    "# 5. Confusion Matrix\n",
    "def confusion_matrix(cluster_indices, cluster_labels, y_test):\n",
    "    y_pred = np.zeros(len(y_test))\n",
    "    for i in range(len(cluster_indices)):\n",
    "        y_pred[cluster_indices[i]] = cluster_labels[i]\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(ConfusionMatrixDisplay(cm).plot())\n",
    "    return cm\n",
    "\n",
    "# 6. Classification Report (Precision, Recall, F1-Score, Support)\n",
    "def classification_report(cluster_indices, cluster_labels, y_test):\n",
    "    y_pred = np.zeros(len(y_test))\n",
    "    for i in range(len(cluster_indices)):\n",
    "        y_pred[cluster_indices[i]] = cluster_labels[i]\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to calculate the euclidean distance between two data points\n",
    "def euclidean_distance( row, centroid, data ) -> float:\n",
    "    distance = 0.0\n",
    "    for column in data.columns:\n",
    "        distance += (row[column] - centroid[column])**2\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "\n",
    "# This function will be used to calculate the mean of the data points in a cluster.\n",
    "def calculate_mean( data ) -> pd.DataFrame:\n",
    "    mean = pd.DataFrame(columns=data.columns)\n",
    "    for column in data.columns:\n",
    "        mean[column] = [data[column].mean()]\n",
    "    return mean\n",
    "\n",
    "# This function saves the kmeans model state using pickle\n",
    "def save_state_kmeans( k: int, centroids: pd.DataFrame, clusters: dict, cluster_indices: dict, cluster_labels: dict ):\n",
    "    with open('checkpoints/centroids' + str(k) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(centroids, file)\n",
    "    with open('checkpoints/clusters' + str(k) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(clusters, file)\n",
    "    with open('checkpoints/cluster_indices' + str(k) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(cluster_indices, file)\n",
    "    with open('checkpoints/cluster_labels' + str(k) + '.pkl', 'wb') as file:\n",
    "        pickle.dump(cluster_labels, file)\n",
    "\n",
    "# This function loads the kmeans model state using pickle\n",
    "def load_state_kmeans( k: int ) -> tuple:\n",
    "    with open('checkpoints/centroids' + str(k) + '.pkl', 'rb') as file:\n",
    "        centroids = pd.DataFrame( pickle.load(file) )\n",
    "    with open('checkpoints/clusters' + str(k) + '.pkl', 'rb') as file:\n",
    "        clusters = dict( pickle.load(file) )\n",
    "    with open('checkpoints/cluster_indices' + str(k) + '.pkl', 'rb') as file:\n",
    "        cluster_indices = dict( pickle.load(file) )\n",
    "    with open('checkpoints/cluster_labels' + str(k) + '.pkl', 'rb') as file:\n",
    "        cluster_labels = dict( pickle.load(file) )\n",
    "    return centroids, clusters, cluster_indices, cluster_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering( k, data, max_iterations:int=None, print_updates=False, initial_centroids=None ):\n",
    "    \n",
    "    # Initially, selecting k random data points as centroids\n",
    "    # We will use the current time as the seed to make sure that we get different centroids each time we run the algorithm\n",
    "    \n",
    "    np.random.seed( 42 )\n",
    "\n",
    "    if initial_centroids is None:\n",
    "        centroids = data.sample( k, random_state= 42 )\n",
    "    else:\n",
    "        centroids = initial_centroids.copy()\n",
    "        \n",
    "    old_centroids = None\n",
    "\n",
    "    # If the user doesn't specify the maximum number of iterations, we will set it to infinity (Loop until convergence)\n",
    "    if max_iterations is None:\n",
    "        max_iterations = np.inf\n",
    "\n",
    "    itr = 1\n",
    "    while( itr <= max_iterations ):\n",
    "\n",
    "        # If the centroids do not change, we will stop the algorithm\n",
    "        if centroids.equals( old_centroids ):\n",
    "            break\n",
    "\n",
    "        # Storing the old centroids to check if they change in the next iteration\n",
    "        old_centroids = centroids.copy()\n",
    "\n",
    "        if print_updates is True: print( underline(bold(' Iteration #' + str(itr) + ' ')) )\n",
    "\n",
    "        # Container initialization for cluster data\n",
    "        clusters = {} # clusters[i] will store the data points in the (i+1)th cluster\n",
    "        cluster_indices = {} # cluster_indices[i] will store the indices of the data points in the (i+1)th cluster\n",
    "        cluster_labels = {} # cluster_labels[i] will store the label of the (i+1)th cluster by majority voting\n",
    "        for i in range(k):\n",
    "            clusters[i] = []\n",
    "            cluster_indices[i] = []\n",
    "\n",
    "        # Iterating through each data point and assigning it to the closest cluster\n",
    "        for index, row in data.iterrows():\n",
    "\n",
    "            min_distance, closest_cluster_index = np.inf, -1\n",
    "            \n",
    "            # Iterating through each centroid to find the closest one\n",
    "            for i in range(k):\n",
    "\n",
    "                # Using our euclidean_distance function to calculate the distance as it handles both numerical and categorical data\n",
    "                current_distance = euclidean_distance( row, centroids.iloc[i], data )\n",
    "\n",
    "                # Check if the data point is closer to the ith centroid\n",
    "                if current_distance < min_distance:\n",
    "                    min_distance = current_distance\n",
    "                    closest_cluster_index = i\n",
    "            \n",
    "            # Assigning the data point to the cluster with the closest centroid\n",
    "            clusters[closest_cluster_index].append( row )\n",
    "            cluster_indices[closest_cluster_index].append( index )\n",
    "\n",
    "        # Updating the centroids.\n",
    "        # We will use our calculate_mean function to calculate the mean of the data points in the cluster\n",
    "        # because it handles both numerical and categorical data\n",
    "        for i in range(k):\n",
    "\n",
    "            # If the cluster is empty, we will not update the centroid\n",
    "            if len(clusters[i]) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                centroids.iloc[i] = calculate_mean( pd.DataFrame(clusters[i]) )\n",
    "\n",
    "        \n",
    "        # Calculating the cluster labels by majority voting (if the cluster is not empty)\n",
    "        for i in range(k):\n",
    "            if len(clusters[i]) == 0:\n",
    "                cluster_labels[i] = None\n",
    "            else:\n",
    "                cluster_labels[i] = pd.Series( [y_train[index] for index in cluster_indices[i]] ).value_counts().index[0]\n",
    "\n",
    "        # Printing the cluster sizes and labels in a pandas table\n",
    "        if print_updates is True:\n",
    "            print( pd.DataFrame( [ [len(clusters[i]), cluster_labels[i]] for i in range(k) ], columns=['Cluster Size', 'Cluster Label'] ) )\n",
    "\n",
    "        if print_updates is True: print('-' * 50) # Just to print a line to separate the iterations\n",
    "\n",
    "        save_state_kmeans( k, centroids, clusters, cluster_indices, cluster_labels )\n",
    "\n",
    "        itr += 1\n",
    "\n",
    "    return centroids, clusters, cluster_indices, cluster_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to calculate the purity of the clusters\n",
    "# Purity is the percentage of data points in a cluster that belong to the same class\n",
    "def calculate_purity( clusters, labels, print_report=False ):\n",
    "    purities = []\n",
    "    for i in range( len(clusters) ):\n",
    "        cluster = clusters[i]\n",
    "\n",
    "        # If the cluster is empty, we will skip it\n",
    "        if len(cluster) == 0: continue\n",
    "\n",
    "        # Converting the cluster to a dataframe so that we can use the value_counts() function\n",
    "        cluster = pd.DataFrame(cluster)\n",
    "        cluster['label'] = labels[cluster.index]\n",
    "\n",
    "        # We will use the value_counts() function to count the number of data points in each class\n",
    "        # and then we will divide it by the total number of data points in the cluster\n",
    "        purities.append( cluster['label'].value_counts()[0] / len(cluster) )\n",
    "\n",
    "    # Normalizing the purity by dividing it by the number of clusters\n",
    "    average_purity = sum(purities) / len(clusters)\n",
    "\n",
    "    if print_report is True:\n",
    "        for i in range(len(purities)):\n",
    "            print('Cluster ', i+1, ' purity: ', purities[i])\n",
    "        print('-'*50)\n",
    "        print('Average Purity: ', average_purity)\n",
    "        print('-'*50)\n",
    "    \n",
    "    return average_purity, purities\n",
    "\n",
    "\n",
    "# This function prints a report of the clusters produced by the k-means algorithm\n",
    "def analyze_clusters( clusters, cluster_indices, cluster_labels, labels ):\n",
    "\n",
    "    # Printing the number of data points in each cluster\n",
    "    for i in range(len(cluster_indices)):\n",
    "        print('Cluster ', i+1, ' contains ', len(cluster_indices[i]), ' data points of class ', cluster_labels[i])\n",
    "    print( '-' * 50 )\n",
    "\n",
    "    # Calculating the purity of the clusters and printing the report\n",
    "    calculate_purity( clusters, labels, print_report=True )\n",
    "\n",
    "    # Printing the count for each unique labels in each cluster horizontally\n",
    "    for i in range(len(cluster_indices)):\n",
    "        print( bold('[Cluster #' + str(i+1) + ']'), ' --> ', textf(cluster_labels[i], COLOR_CYAN) )\n",
    "        print(labels[cluster_indices[i]].value_counts())\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: Uncomment the following line to load the saved state\n",
    "# centroids7, clusters7, cluster_indices7, cluster_labels7 = load_state_kmeans( 7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1m Iteration #1 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0        300767        smurf.\n",
      "1             0          None\n",
      "2             0          None\n",
      "3         83600       normal.\n",
      "4         92831      neptune.\n",
      "5             0          None\n",
      "6         16823      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #2 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         18092       normal.\n",
      "1        280256        smurf.\n",
      "2             0          None\n",
      "3         84812       normal.\n",
      "4         25048      neptune.\n",
      "5             0          None\n",
      "6         85813      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #3 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         21011       normal.\n",
      "1         27685        smurf.\n",
      "2        253586        smurf.\n",
      "3         76526       normal.\n",
      "4         28326      neptune.\n",
      "5             0          None\n",
      "6         86887      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #4 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20987       normal.\n",
      "1         19011        smurf.\n",
      "2         35859        smurf.\n",
      "3         76428       normal.\n",
      "4         28237      neptune.\n",
      "5        226449        smurf.\n",
      "6         87050      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #5 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20984       normal.\n",
      "1         13560        smurf.\n",
      "2         40810        smurf.\n",
      "3         76384       normal.\n",
      "4         28223      neptune.\n",
      "5        226998        smurf.\n",
      "6         87062      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #6 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20984       normal.\n",
      "1         11349        smurf.\n",
      "2         11359        smurf.\n",
      "3         76227       normal.\n",
      "4         28218      neptune.\n",
      "5        258822        smurf.\n",
      "6         87062      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #7 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20987       normal.\n",
      "1          1392       normal.\n",
      "2         18846        smurf.\n",
      "3         76071       normal.\n",
      "4         28218      neptune.\n",
      "5        261445        smurf.\n",
      "6         87062      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #8 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20989       normal.\n",
      "1          1839      ipsweep.\n",
      "2         19029        smurf.\n",
      "3         74775       normal.\n",
      "4         28218      neptune.\n",
      "5        262109        smurf.\n",
      "6         87062      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #9 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20998       normal.\n",
      "1          1678      ipsweep.\n",
      "2         14877        smurf.\n",
      "3         74776       normal.\n",
      "4         28218      neptune.\n",
      "5        266412        smurf.\n",
      "6         87062      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #10 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20994       normal.\n",
      "1          1681      ipsweep.\n",
      "2         13117        smurf.\n",
      "3         74760       normal.\n",
      "4         28218      neptune.\n",
      "5        268187        smurf.\n",
      "6         87064      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #11 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20992       normal.\n",
      "1          1687      ipsweep.\n",
      "2         12613        smurf.\n",
      "3         74760       normal.\n",
      "4         28218      neptune.\n",
      "5        268693        smurf.\n",
      "6         87058      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #12 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20992       normal.\n",
      "1          1688      ipsweep.\n",
      "2         12067        smurf.\n",
      "3         74760       normal.\n",
      "4         28218      neptune.\n",
      "5        269239        smurf.\n",
      "6         87057      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #13 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20992       normal.\n",
      "1          1688      ipsweep.\n",
      "2         11672        smurf.\n",
      "3         74760       normal.\n",
      "4         28218      neptune.\n",
      "5        269634        smurf.\n",
      "6         87057      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #14 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20992       normal.\n",
      "1          1688      ipsweep.\n",
      "2         11411        smurf.\n",
      "3         74760       normal.\n",
      "4         28218      neptune.\n",
      "5        269895        smurf.\n",
      "6         87057      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #15 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20991       normal.\n",
      "1          1688      ipsweep.\n",
      "2         11128        smurf.\n",
      "3         74760       normal.\n",
      "4         28218      neptune.\n",
      "5        270179        smurf.\n",
      "6         87057      neptune.\n",
      "--------------------------------------------------\n",
      "\u001b[4m\u001b[1m Iteration #16 \u001b[0m\u001b[0m\n",
      "   Cluster Size Cluster Label\n",
      "0         20991       normal.\n",
      "1          1688      ipsweep.\n",
      "2         11128        smurf.\n",
      "3         74760       normal.\n",
      "4         28218      neptune.\n",
      "5        270179        smurf.\n",
      "6         87057      neptune.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    centroids7, clusters7, cluster_indices7, cluster_labels7 = kmeans_clustering( k=7, data=X_train, print_updates=True )\n",
    "except KeyboardInterrupt:\n",
    "    print( textf('Process interrupted by user', COLOR_RED) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  1  contains  20991  data points of class  normal.\n",
      "Cluster  2  contains  1688  data points of class  ipsweep.\n",
      "Cluster  3  contains  11128  data points of class  smurf.\n",
      "Cluster  4  contains  74760  data points of class  normal.\n",
      "Cluster  5  contains  28218  data points of class  neptune.\n",
      "Cluster  6  contains  270179  data points of class  smurf.\n",
      "Cluster  7  contains  87057  data points of class  neptune.\n",
      "--------------------------------------------------\n",
      "Cluster  1  purity:  0.9417845743413844\n",
      "Cluster  2  purity:  0.6824644549763034\n",
      "Cluster  3  purity:  0.9535406182602444\n",
      "Cluster  4  purity:  0.9544141252006421\n",
      "Cluster  5  purity:  0.7249627897086965\n",
      "Cluster  6  purity:  1.0\n",
      "Cluster  7  purity:  0.9963816809676419\n",
      "--------------------------------------------------\n",
      "Average Purity:  0.8933640347792732\n",
      "--------------------------------------------------\n",
      "\u001b[1m[Cluster #1]\u001b[0m  -->  \u001b[36mnormal.\u001b[0m\n",
      "normal.         19769\n",
      "teardrop.         979\n",
      "satan.            172\n",
      "nmap.              25\n",
      "portsweep.         25\n",
      "pod.               15\n",
      "warezclient.        3\n",
      "rootkit.            3\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "\u001b[1m[Cluster #2]\u001b[0m  -->  \u001b[36mipsweep.\u001b[0m\n",
      "ipsweep.            1152\n",
      "normal.              409\n",
      "nmap.                102\n",
      "land.                 21\n",
      "satan.                 2\n",
      "guess_passwd.          1\n",
      "buffer_overflow.       1\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "\u001b[1m[Cluster #3]\u001b[0m  -->  \u001b[36msmurf.\u001b[0m\n",
      "smurf.        10611\n",
      "normal.         345\n",
      "pod.            166\n",
      "ipsweep.          4\n",
      "nmap.             1\n",
      "portsweep.        1\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "\u001b[1m[Cluster #4]\u001b[0m  -->  \u001b[36mnormal.\u001b[0m\n",
      "normal.             71352\n",
      "back.                2196\n",
      "warezclient.         1016\n",
      "pod.                   83\n",
      "buffer_overflow.       29\n",
      "warezmaster.           20\n",
      "satan.                  9\n",
      "loadmodule.             9\n",
      "ftp_write.              8\n",
      "imap.                   7\n",
      "multihop.               7\n",
      "rootkit.                7\n",
      "ipsweep.                5\n",
      "phf.                    4\n",
      "perl.                   3\n",
      "spy.                    2\n",
      "neptune.                2\n",
      "guess_passwd.           1\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "\u001b[1m[Cluster #5]\u001b[0m  -->  \u001b[36mneptune.\u001b[0m\n",
      "neptune.         20457\n",
      "normal.           5397\n",
      "satan.            1233\n",
      "portsweep.         987\n",
      "ipsweep.            86\n",
      "guess_passwd.       50\n",
      "back.                7\n",
      "warezclient.         1\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "\u001b[1m[Cluster #6]\u001b[0m  -->  \u001b[36msmurf.\u001b[0m\n",
      "smurf.    270179\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "\u001b[1m[Cluster #7]\u001b[0m  -->  \u001b[36mneptune.\u001b[0m\n",
      "neptune.         86742\n",
      "satan.             173\n",
      "nmap.              103\n",
      "portsweep.          27\n",
      "normal.              6\n",
      "imap.                5\n",
      "guess_passwd.        1\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    analyze_clusters( clusters7, cluster_indices7, cluster_labels7, y_train )\n",
    "except NameError:\n",
    "    print( textf('NameError: undefined arguments in analyze_clusters()', COLOR_RED) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    centroids15, clusters15, cluster_indices15, cluster_labels15 = kmeans_clustering( k=15, data=X_test, print_updates=True )\n",
    "except KeyboardInterrupt:\n",
    "    print( textf('Process interrupted by user', COLOR_RED) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analyze_clusters( clusters15, cluster_indices15, cluster_labels15, y_test )\n",
    "except NameError:\n",
    "    print( textf('NameError: undefined arguments in analyze_clusters()', COLOR_RED) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    centroids23, clusters23, cluster_indices23, cluster_labels23 = kmeans_clustering( k=23, data=X_test, print_updates=True )\n",
    "except KeyboardInterrupt:\n",
    "    print( textf('Process interrupted by user', COLOR_RED) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analyze_clusters( clusters23, cluster_indices23, cluster_labels23, y_test )\n",
    "except NameError:\n",
    "    print( textf('NameError: undefined arguments in analyze_clusters()', COLOR_RED) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    centroids31, clusters31, cluster_indices31, cluster_labels31 = kmeans_clustering( k=31, data=X_test, print_updates=True )\n",
    "except KeyboardInterrupt:\n",
    "    print( textf('Process interrupted by user', COLOR_RED) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analyze_clusters( clusters31, cluster_indices31, cluster_labels31, y_test )\n",
    "except NameError:\n",
    "    print( textf('NameError: undefined arguments in analyze_clusters()', COLOR_RED) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    centroids45, clusters45, cluster_indices45, cluster_labels45 = kmeans_clustering( k=31, data=X_test, print_updates=True )\n",
    "except KeyboardInterrupt:\n",
    "    print( textf('Process interrupted by user', COLOR_RED) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analyze_clusters( clusters45, cluster_indices45, cluster_labels45, y_test )\n",
    "except NameError:\n",
    "    print( textf('NameError: undefined arguments in analyze_clusters()', COLOR_RED) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Cut Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation (Not vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "def constructWeightMatrix(data, gamma = 1):\n",
    "\n",
    "    # Calculating the distance matrix\n",
    "    distance_matrix = euclidean_distances( data, data )\n",
    "\n",
    "    # Calculating the weight matrix\n",
    "    weight_matrix = np.exp( -gamma * (distance_matrix**2) )\n",
    "\n",
    "    # Returning the weight matrix\n",
    "    return weight_matrix\n",
    "\n",
    "\n",
    "def measure_cut( weight_matrix, clusters ):\n",
    "    # Calculating the cut\n",
    "    # Here, we will iterate through each pair of clusters and calculate the sum of the weights of the edges between them\n",
    "    # We add the cut between the ith and jth clusters to the total cut measure which will then be used to calculate the normalized cut\n",
    "    total_cut_measure = 0\n",
    "    for i in range( len(clusters) ):\n",
    "        for j in range( len(clusters) ):\n",
    "\n",
    "            # We don't want to calculate the cut between a cluster and itself\n",
    "            # So, we will skip the iteration if i == j\n",
    "            if i == j: continue\n",
    "\n",
    "            # Calculating the cut between the ith and jth clusters\n",
    "            cut = np.sum( weight_matrix[clusters[i], :][:, clusters[j]] )\n",
    "            total_cut_measure += cut\n",
    "            \n",
    "    # Calculating the volume\n",
    "    # Here, we will iterate through each cluster and calculate the sum of the weights of the edges inside the cluster\n",
    "    total_volume = 0\n",
    "    for i in range( len(clusters) ):\n",
    "        total_volume += np.sum( weight_matrix[clusters[i], :][:, clusters[i]] )\n",
    "\n",
    "    # Calculating the normalized cut\n",
    "    normalized_cut = total_cut_measure / total_volume\n",
    "\n",
    "    return normalized_cut, total_cut_measure\n",
    "\n",
    "def clusteringUsingNormalizedCut(data, k, gamma = 1):\n",
    "\n",
    "    # Constructing the weight matrix\n",
    "    weight_matrix = constructWeightMatrix(data, gamma=gamma)\n",
    "\n",
    "    # Computing the degree matrix\n",
    "    degree_matrix = np.diag( np.sum(weight_matrix, axis=1) )\n",
    "\n",
    "    # Computing the laplacian matrix\n",
    "    laplacian_matrix = degree_matrix - weight_matrix\n",
    "\n",
    "    # Computing the eigenvalues and eigenvectors of the laplacian matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(np.dot(np.linalg.inv(degree_matrix), laplacian_matrix))\n",
    "    \n",
    "    # Taking the only first k eigenvectors\n",
    "    eigenvectors = eigenvectors[:, :k]\n",
    "\n",
    "    # Applying k-means on the eigenvectors\n",
    "    centroids, clusters, cluster_indices, cluster_labels = kmeans_clustering(k=k, data=eigenvectors, print_updates=False)\n",
    "    \n",
    "    # Evaluating the clusters using the normalized cut\n",
    "    normalized_cut, total_cut_measure = measure_cut(weight_matrix, clusters)\n",
    "    \n",
    "    return centroids, clusters, cluster_indices, cluster_labels, normalized_cut, total_cut_measure"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering (Agglomerative Clustering not Divisive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix as distanceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkage(distance_matrix, clusters, cluster1, cluster2, linkage):\n",
    "    if linkage == 'single':\n",
    "        min_distance = np.inf\n",
    "        for i in clusters[cluster1]:\n",
    "            for j in clusters[cluster2]:\n",
    "                if distance_matrix[i,j] < min_distance:\n",
    "                    min_distance = distance_matrix[i,j]\n",
    "        return min_distance\n",
    "    elif linkage == 'complete':\n",
    "        max_distance = -np.inf\n",
    "        for i in clusters[cluster1]:\n",
    "            for j in clusters[cluster2]:\n",
    "                if distance_matrix[i,j] > max_distance:\n",
    "                    max_distance = distance_matrix[i,j]\n",
    "        return max_distance\n",
    "    elif linkage == 'average' or linkage == 'centroid' or linkage == 'mean':\n",
    "        sum_distance = 0\n",
    "        for i in clusters[cluster1]:\n",
    "            for j in clusters[cluster2]:\n",
    "                sum_distance += distance_matrix[i,j]\n",
    "        return sum_distance/(len(clusters[cluster2]) * len(clusters[cluster1]))\n",
    "    else:\n",
    "        raise ValueError('Invalid linkage type')\n",
    "    \n",
    "def hierarchical_clustering(data, linkage_type, n_clusters):\n",
    "    \n",
    "    # Marking each data point as a cluster\n",
    "    clusters = [[i] for i in range(len(data))]\n",
    "\n",
    "    # Calculating the distance matrix using Euclidean distance (default of distanceMatrix function)\n",
    "    distance_matrix = distanceMatrix(data, data)\n",
    "\n",
    "    # Iteratively merging the clusters\n",
    "    while len(clusters) > n_clusters:\n",
    "\n",
    "        # Finding closest two clusters\n",
    "        min_distance = np.inf\n",
    "        min_i,min_j = -1,-1\n",
    "        for i in range(len(clusters)):\n",
    "            for j in range(i+1,len(clusters)):\n",
    "                dist = distance_matrix[i,j]\n",
    "                if dist < min_distance:\n",
    "                    min_distance = dist\n",
    "                    min_i,min_j = i,j\n",
    "\n",
    "        # Merging the closest two clusters\n",
    "        clusters[min_i] = clusters[min_i] + clusters[min_j]\n",
    "\n",
    "        # Removing the second cluster\n",
    "        clusters.pop(min_j)\n",
    "        \n",
    "        # Adjusting the distance matrix \n",
    "        for i in range(len(clusters)):\n",
    "            if i == min_i: continue\n",
    "            distance_matrix[min_i,i] = distance_matrix[i,min_i] = linkage(distance_matrix, clusters, i, min_i, linkage_type)\n",
    "\n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]\n"
     ]
    }
   ],
   "source": [
    "# Testing the hierarchical clustering function\n",
    "data = [[1,2],[3,4],[5,6],[7,8],[9,10],[11,12],[13,14],[15,16],[17,18],[19,20]]\n",
    "clusters = hierarchical_clustering(data, 'mean', 5)\n",
    "print(clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92e83001589606ab90afa46080212dcb734f30c24d2f66d52b7ead7b1f127be3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
