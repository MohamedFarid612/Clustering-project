{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection using K-Means and Spectral Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First: Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second: Importing the dataset and preprocessing it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We first load the data from the CSVs.\n",
    "- We then proceed with preprocessing the data as follows:\n",
    "    1. Split the labels from the data (last column) and place them in separate dataframes.\n",
    "    2. Encode the categorical data using into one-hot vectors using the OneHotEncoder class.\n",
    "    3. Scale the features using the StandardScaler class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data as indicated in the assignment\n",
    "# kddcup.data.corrected is the training data, corrected is the testing data\n",
    "training_data = pd.read_csv( 'archive/kddcup.data.corrected', header=None )\n",
    "testing_data = pd.read_csv( 'archive/corrected', header=None )\n",
    "\n",
    "# Separating the labels (last column) from the features in the training and testing data\n",
    "X_train = training_data.iloc[:, :-1]\n",
    "y_train = training_data.iloc[:, -1]\n",
    "\n",
    "X_test = testing_data.iloc[:, :-1]\n",
    "y_test = testing_data.iloc[:, -1]\n",
    "\n",
    "# Concatenating the training and testing data vertically to perform OneHotEncoding on all the categorical features\n",
    "X = pd.concat( [X_train, X_test] )\n",
    "\n",
    "# Encoding the features into one-hot vectors using OneHotEncoder\n",
    "# ColumnTransformer is used to apply the OneHotEncoder to the second, third and fourth columns (categorical features)\n",
    "# The remainder is set to 'passthrough' to keep the other columns unchanged (numerical features)\n",
    "col_trans = ColumnTransformer( transformers=[('encoder', OneHotEncoder(), [1, 2, 3])], remainder='passthrough' )\n",
    "X = col_trans.fit_transform(X)\n",
    "\n",
    "# Separating the training and testing data again after OneHotEncoding\n",
    "X_train = X[:len(X_train)]\n",
    "X_test = X[len(X_train):]\n",
    "\n",
    "# Feature Scaling since some features have a much higher range than others\n",
    "scaler = StandardScaler()\n",
    "X_train = pd.DataFrame( scaler.fit_transform( X_train ) )\n",
    "X_test = pd.DataFrame( scaler.transform( X_test ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly picking 10000 samples from the training data to speed up the training process (for testing purposes only)\n",
    "X_train = X_train.sample(10000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels in the training data:  23\n",
      "------------------------------\n",
      "Number of unique labels in the testing data:  38\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Number of unique labels in the training data: ', len(y_train.unique()))\n",
    "    print('-' * 30)\n",
    "    print('Number of unique labels in the testing data: ', len(y_test.unique()))\n",
    "except NameError:\n",
    "    print('\\033[91m' + 'NameError: y_train or y_test is not defined' + '\\033[0m')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third: Applying K-Means and Normalized Cut"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to calculate the euclidean distance between two data points\n",
    "def euclidean_distance( row, centroid, data ):\n",
    "    distance = 0.0\n",
    "    for column in data.columns:\n",
    "        distance += (row[column] - centroid[column])**2\n",
    "    return np.sqrt(distance)\n",
    "\n",
    "\n",
    "# This function will be used to calculate the mean of the data points in a cluster.\n",
    "def calculate_mean( data ):\n",
    "    mean = pd.DataFrame(columns=data.columns)\n",
    "    for column in data.columns:\n",
    "        mean[column] = [data[column].mean()]\n",
    "    return mean\n",
    "    \n",
    "\n",
    "# This function returns the ANSI code for bold text\n",
    "def bold( text, reset=True ):\n",
    "    if reset:\n",
    "        return '\\033[1m' + text + '\\033[0m'\n",
    "    return '\\033[1m' + text\n",
    "\n",
    "# This function returns the ANSI code for underlined text\n",
    "def underline( text, reset=True ):\n",
    "    if reset:\n",
    "        return '\\033[4m' + text + '\\033[0m'\n",
    "    return '\\033[4m' + text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_clustering( k, data, max_iterations:int=None, print_updates=False, initial_centroids=None ):\n",
    "    \n",
    "    # Initially, selecting k random data points as centroids\n",
    "    # We will use the current time as the seed to make sure that we get different centroids each time we run the algorithm\n",
    "    np.random.seed( int(time.time()) )\n",
    "\n",
    "    if initial_centroids is None:\n",
    "        centroids = data.sample(k)\n",
    "    else:\n",
    "        centroids = initial_centroids.copy()\n",
    "        \n",
    "    old_centroids = None\n",
    "\n",
    "    # If the user doesn't specify the maximum number of iterations, we will set it to infinity (Loop until convergence)\n",
    "    if max_iterations is None:\n",
    "        max_iterations = np.inf\n",
    "\n",
    "    itr = 1\n",
    "    while( itr <= max_iterations ):\n",
    "\n",
    "        # If the centroids do not change, we will stop the algorithm\n",
    "        if centroids.equals( old_centroids ):\n",
    "            break\n",
    "\n",
    "        # Storing the old centroids to check if they change in the next iteration\n",
    "        old_centroids = centroids.copy()\n",
    "\n",
    "        if print_updates is True: print( underline(bold('Iteration #' + str(itr))) )\n",
    "\n",
    "        # Creating a dictionary to store the clusters and their data points\n",
    "        # 'clusters' will store the data points and 'cluster_indices' will store their indices\n",
    "        clusters, cluster_indices = {}, {}\n",
    "        for i in range(k):\n",
    "            clusters[i] = []\n",
    "            cluster_indices[i] = []\n",
    "\n",
    "        # Iterating through each data point and assigning it to the closest cluster\n",
    "        for index, row in data.iterrows():\n",
    "\n",
    "            min_distance, closest_cluster_index = np.inf, -1\n",
    "            \n",
    "            # Iterating through each centroid to find the closest one\n",
    "            for i in range(k):\n",
    "\n",
    "                # Using our euclidean_distance function to calculate the distance as it handles both numerical and categorical data\n",
    "                current_distance = euclidean_distance( row, centroids.iloc[i], data )\n",
    "\n",
    "                # Check if the data point is closer to the ith centroid\n",
    "                if current_distance < min_distance:\n",
    "                    min_distance = current_distance\n",
    "                    closest_cluster_index = i\n",
    "            \n",
    "            # Assigning the data point to the cluster with the closest centroid\n",
    "            clusters[closest_cluster_index].append( row )\n",
    "            cluster_indices[closest_cluster_index].append( index )\n",
    "\n",
    "        # Updating the centroids.\n",
    "        # We will use our calculate_mean function to calculate the mean of the data points in the cluster\n",
    "        # because it handles both numerical and categorical data\n",
    "        for i in range(k):\n",
    "\n",
    "            # If the cluster is empty, we will not update the centroid\n",
    "            if len(clusters[i]) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                centroids.iloc[i] = calculate_mean( pd.DataFrame(clusters[i]) )\n",
    "\n",
    "        # Printing the cluster sizes in tabular form to conserve vertical space\n",
    "        if print_updates is True:\n",
    "            print( 'Cluster sizes:' )\n",
    "            print(pd.DataFrame( [len(clusters[i]) for i in range(k)] ).T)\n",
    "\n",
    "        if print_updates is True: print('-' * 50) # Just to print a line to separate the iterations\n",
    "\n",
    "        itr += 1\n",
    "\n",
    "    return centroids, clusters, cluster_indices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used to calculate the purity of the clusters\n",
    "# Purity is the percentage of data points in a cluster that belong to the same class\n",
    "def calculate_purity( clusters, labels, print_report=False ):\n",
    "    purity = 0.0\n",
    "    purities = []\n",
    "    for i in range( len(clusters) ):\n",
    "        cluster = clusters[i]\n",
    "\n",
    "        # If the cluster is empty, we will skip it\n",
    "        if len(cluster) == 0: continue\n",
    "\n",
    "        # Converting the cluster to a dataframe so that we can use the value_counts() function\n",
    "        cluster = pd.DataFrame(cluster)\n",
    "        cluster['label'] = labels[cluster.index]\n",
    "\n",
    "        # We will use the value_counts() function to count the number of data points in each class\n",
    "        # and then we will divide it by the total number of data points in the cluster\n",
    "        purities.append( cluster['label'].value_counts()[0] / len(cluster) )\n",
    "\n",
    "    # Normalizing the purity by dividing it by the number of clusters\n",
    "    average_purity = sum(purities) / len(clusters)\n",
    "\n",
    "    if print_report is True:\n",
    "        for i in range(len(purities)):\n",
    "            print('Cluster ', i+1, ' purity: ', purities[i])\n",
    "        print('-'*50)\n",
    "        print('Average Purity: ', average_purity)\n",
    "        print('-'*50)\n",
    "    \n",
    "    return average_purity, purities\n",
    "\n",
    "\n",
    "# This function prints a report of the clusters produced by the k-means algorithm\n",
    "def analyze_clusters( clusters, cluster_indices, labels ):\n",
    "\n",
    "    # Printing the number of data points in each cluster\n",
    "    for i in range(len(cluster_indices)):\n",
    "        print('Cluster ', i+1, ' contains ', len(cluster_indices[i]), ' data points')\n",
    "    print( '-' * 50 )\n",
    "\n",
    "    # Calculating the purity of the clusters and printing the report\n",
    "    calculate_purity( clusters, labels, print_report=True )\n",
    "\n",
    "    # Printing the number of unique labels in each cluster\n",
    "    for i in range(len(cluster_indices)):\n",
    "        print( 'Cluster #' + str(i+1) + ' labels:\\n' )\n",
    "        print(labels[cluster_indices[i]].value_counts())\n",
    "        print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[1mIteration #1\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    centroids2, clusters2, cluster_indices2 = kmeans_clustering( k=10, data=X_train, print_updates=True )\n",
    "except KeyboardInterrupt:\n",
    "    print('\\033[91m' + 'Process interrupted by user' + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  1  contains  5708  data points\n",
      "Cluster  2  contains  4292  data points\n",
      "--------------------------------------------------\n",
      "Cluster  1  purity:  0.9973721093202523\n",
      "Cluster  2  purity:  0.5263280521901211\n",
      "--------------------------------------------------\n",
      "Average Purity:  0.7618500807551867\n",
      "--------------------------------------------------\n",
      "Cluster #1 labels:\n",
      "\n",
      "smurf.      5693\n",
      "normal.       14\n",
      "ipsweep.       1\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n",
      "Cluster #2 labels:\n",
      "\n",
      "neptune.        2259\n",
      "normal.         1950\n",
      "satan.            29\n",
      "portsweep.        22\n",
      "ipsweep.          18\n",
      "back.              6\n",
      "nmap.              4\n",
      "teardrop.          2\n",
      "warezclient.       2\n",
      "Name: 41, dtype: int64\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    analyze_clusters( clusters2, cluster_indices2, y_train )\n",
    "except NameError:\n",
    "    print('\\033[91m' + 'NameError: clusters2 or cluster_indices2 is not defined' + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    centroids23, clusters23, cluster_indices23 = kmeans_clustering( k=23, data=X_test, max_iterations=5, print_updates=True )\n",
    "except KeyboardInterrupt:\n",
    "    print('\\033[91m' + 'Process interrupted by user' + '\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analyze_clusters( clusters23, cluster_indices23, y_test )\n",
    "except NameError:\n",
    "    print('\\033[91m' + 'Error: No clusters found' + '\\033[0m')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Cut Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation (Not vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function measures the normalized cut of a graph, given the weight matrix and the clusters resulting from the graph cut\n",
    "# weight_matrix expects a numpy array, and clusters expects a list of lists where each list contains the indices of the nodes in the cluster\n",
    "def measure_cut( weight_matrix, clusters ):\n",
    "    \n",
    "    # Calculating the cut\n",
    "    # Here, we will iterate through each pair of clusters and calculate the sum of the weights of the edges between them\n",
    "    # We add the cut between the ith and jth clusters to the total cut measure which will then be used to calculate the normalized cut\n",
    "    total_cut_measure = 0\n",
    "    for i in range( len(clusters) ):\n",
    "        for j in range( len(clusters) ):\n",
    "\n",
    "            # We don't want to calculate the cut between a cluster and itself\n",
    "            # So, we will skip the iteration if i == j\n",
    "            if i == j: continue\n",
    "\n",
    "            # Calculating the cut between the ith and jth clusters\n",
    "            cut += np.sum( weight_matrix[clusters[i], :][:, clusters[j]] )\n",
    "\n",
    "    # Calculating the volume\n",
    "    # Here, we will iterate through each cluster and calculate the sum of the weights of the edges inside the cluster\n",
    "    total_volume = 0\n",
    "    for i in range( len(clusters) ):\n",
    "        total_volume += np.sum( weight_matrix[clusters[i], :][:, clusters[i]] )\n",
    "\n",
    "    # Calculating the normalized cut\n",
    "    normalized_cut = total_cut_measure / total_volume\n",
    "\n",
    "    return normalized_cut, total_cut_measure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
